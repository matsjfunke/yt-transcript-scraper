if you watched my previous video you  probably understand how the computer is  capable of classifying images this video  will aim to give you an in-depth  understanding of how the forward  propagation in a convolutional neural  network works by visualizing and  describing the mathematics that goes on  under the hood  [Music]  before we get started let's have a quick  look at the model where we skim through  the different processes feel free to  skip this part  the model is pre-trained and has an  accuracy of 95.4 percent it will take in  an input image with the Dimensions 28 by  28 by 1. first off we will perform a  convolution with two filters that have a  filter size of Pi by pi by one and use  an activation function called value  next we will perform Max pooling with  the pull size of 2x2 and the stride of  2. this is our first layer then we move  on performing our second convolution  this time we will use poor filters with  the dimensions three by three by two the  activation function is sigmoid after  this we will perform Max pooling as we  did earlier with the pull size of 2x2  and the Sprite of two  then we flatten out the matrices this is  our second layer  finally we have a fully connected layer  which outputs the predictions now let us  see the model in action we start with  the input layer which is a 28 by 28 by  one dimensional image these kinds of  images are also referred to as grayscale  images as they don't have color channels  such as RGB images does  don't worry though the process is the  same if we break down the image to the  pixel level we see that each pixel is  just a number in the range from 0 to 255  where 0 is Pitch Black while 255 is  white  the image is fed into the first  convolutional layer which consists of  two five by five by one dimensional  filters these filters are often referred  to as kernels each filter also have  their own bias term  we will use the stride of one and an  activation function called relu  the activations from the first filter  will be mapped onto an activation layer  now let's have a deeper look at what  actually goes on during each step of the  convolution  in order to get the activation value  which is the pixel you see on the  activation layer we use this formula  now  let's break it down  since we use the strata one we Slide the  kernel one step to the right on each  iteration until we reach the end then we  will slide it one step down and all the  way back to the beginning  anyways if we look at the first term of  the formula this basically tells us to  multiply the weights from the kernel  with the overlapping values from the  input  this will return a matrix with the same  dimensions  then we'll sum up all the values in that  Matrix and add the bias term in order to  get z183  notice that this is  z183 as it is the  183rd iteration which basically means  that we have split the kernel  182 times before this  in order to get the activation value we  pass z183 to the relief function  value stands for rectified linear unit  and this is the function  we can visualize it to get a better  understanding  simply for zero or any negative value  value will return 0 for any positive  value value will return the inputted  value  if we go back to our activation which is  negative  41.1665 we notice that Prelude outputs  zero since that is a negative number in  visual terms a black pixel  we then slide the kernel one step to the  right and repeat the process  doing this but a whole image will give  us the final activation layer from the  first filter  then you'll do the exact same thing for  the second filter and move on to the  pooling layer  as you can see the first convolution  outputted two images or matrices these  are called channels we will now perform  Max pooling with a pull size of 2x2 and  the stride of two the pool size is  exactly the same as the filter size you  will have a matrix with the dimensions  2x2 performing Max pooling on the first  channel will result in this now let's  delve deeper into the cooling process to  see what's actually going on we start  off by placing the filter in the upper  left corner exactly as we did with the  convolutional layer but instead of doing  any heavy calculations we simply extract  the maximum value and Slide the kernel  two steps to the right since we use the  Strider too we will then repeat the  process and extract the maximum value  you might have heard of mint pooling and  average pulling basically for mean  pulling you will as the name suggests  choose the minimum value well for  average pooling you will choose the  average of the Matrix in this case the  average of the four values the filter  slides over moving back to our example  we will continue with the same process  until we have covered the whole Channel  effectively by choosing a filter size of  2x2 and the stride of 2 we have Shrunk  the dimensions from 24 by 24 by 2 into  12 by 12 by 2. basically half the  dimensions of each of the channels  next up is our second convolutional  layer here we will use four filters with  the stratus 1 and an activation function  called sigmoid a as you're probably  noticed by now these filters are stacked  if you separate them you'll see that we  actually have eight filters so what is  going on  well there are four filters with the  dimensions three by three by two since  we have two channels from the previous  output which is this layer's input will  have two channels for each filter in  other words four filters with two  channels there is also a bias term for  each of these filters  running the convolution outputs and  activation similar to the first  convolution but since we have multiple  channels the calculation becomes  slightly different let's have a look  first we will place each kernel over  their respective channel in the upper  left corner just as we did earlier and  write out the formula for calculating  the activation  as you can see the formula is slightly  different but still very much the same  we will start with the first term which  basically tells us to multiply the  kernel with the values from the input  layer this goes for both kernels  The Next Step tells us to add all the  values from both matrices together now  you could do this in multiple ways such  as adding each term together one by one  but in this example we will add the  matrices together first and then sum up  all the values the results would have  been the same either way next in order  to get Z1 we will have to add the bias  term  then we move on to calculating the  activation which tells us to take the  sigmoid of Z1 which in our case is  negative 7.36 if you are not familiar  with the sigmoid function we will have a  quick look at the function by looking at  the function or the graph you might  notice that the sigmoid outputs a value  in the range between approximately 0 to  approximately 1.  calculating the sigmoid for our Z1  returns approximately zero we can see  that by plotting Z1 on the graph this  means that the activation is equal to  zero  we will plot A1 to the activation layer  and move the kernel one step to the  right and repeat the process again and  again until we cover everything  this process is repeated for each of the  filters you might notice that some of  the pixels on the activation layer are  super wide while the activation value  only ranges from 0 to 1 due to the  sigmoid function  this is for visual purposes only if we  chose the conventional way of  visualizing 255 as white and zero as  black we wouldn't have been able to see  anything on the activation layers  anyways let's move on  this is our last pulling layer which is  exactly the same as the previous one  we'll use a pool size of 2x2 and The  Strider too this will have the  dimensions for each of the channels and  extract the most activated pixels we'll  then take the four matrices and flatten  them out we have now gone from four  matrices that were 5x5 to a flat layer  with 100 nodes each node corresponds to  a pixel  in order to make the visualization  easier for the next step we'll hide all  the nodes except the first and last five  finally we have the output layer where  we want to Output one of ten different  predictions this is the fully connected  layer let's have a quick look at how  these layers work  in our model we have 100 input nodes and  10 output nodes to simplify the  explanation process we will scale  everything down to three input and two  output nodes  fully connected layer consists of input  nodes and output nodes that are  connected with weights and biases  passing the input through the weights  and biases will yield an output  first of all usually each output node  has a bias term that will kick in later  in the process  then we have the input node which is  connected to the output nodes through  some weights these weights are just  numbers  you multiply the input node with the  corresponding weight and add it to the  output you will do that for every output  node and move on to the next input node  the process is the same you multiply the  input node with the weight and add it to  the corresponding output node before  moving on to the final input  here the same process is applied for the  third input node before finally adding  the bias terms you might think that this  is a tedious problem for bigger models  and it is therefore matrix  multiplication is used to tackle the  calculations so here is the formula the  output Matrix Z which in our case would  be 3.6 and 7 is defined as the matrix  multiplication of the input nodes times  the weights plus the bias terms  now you could add an activation such as  value or sigmoid Etc to this output but  this wasn't done in our model as this  was the last player let's get back if we  scale everything we just saw up and run  the input nodes through the final fully  connected layer we'll see that the model  predicts that the inputted image in the  beginning is 7. exactly what I thought  as well  please make sure to like comment and  subscribe to the channel if you got some  value out of this video
